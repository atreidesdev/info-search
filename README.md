# Делали:

* Моисеенко Дмитрий (11-104)
* Харин Ильдар (11-104)

---

## Задание 1: 


1. Скачать минимум 100 текстовых страниц с помощью краулера из предварительно подготовленного списка
- список страниц, сайтов можно найти в интернете
- каждая страница должна содержать текст (ссылки на js, css файлы недопустимы)
- язык текста  должен быть одинаков для всех страниц

2. Записать каждую страницу в текстовый файл ("выкачка")
- очищать выкачку от html разметки  НЕ надо(выкачиваем вместе с разметкой)

3. Создать файл index.txt в котором хранится номер файла и ссылка на страницу

---

### Инструкция:

0. Создать виртуальное окружение - `python -m venv venv`
1. Активировать виртуальное окружение - `source venv/bin/activate`
2. Установить зависимости - `pip install -r requirements.txt`
3. Запустить краулер - `python crawler.py`

---

## Задание 2


1. Из сохраненных документов выделить отдельные слова (токенизация) и получить список токенов

- список не должен содержать дубликатов, союзов, предлогов, чисел
- список не должен содержать "мусора" (слов содержащих одновременно буквы и цифры, обрывки разметки и тд.)
- язык текста должен быть одинаков для всех страниц

2. Сгруппировать токены по леммам

---

### Инструкция:

0. Создать виртуальное окружение - `python -m venv venv`
1. Активировать виртуальное окружение - `source venv/bin/activate`
2. Установить зависимости - `pip install -r requirements.txt`
3. Запустить краулер - `python crawler.py`
4. Запустить организатор файлов - `python organizer.py`
5. Запустить токенизатор - `python tokenizer.py`

